{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1yRUPdv8g58x4h6ffn1XRpjC9Whco_B8G",
      "authorship_tag": "ABX9TyMuO7AT9FfARzIy0bOVU1XU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wcliao1962/2025_DL/blob/master/CrossEntropy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **交叉熵損失函數**\n",
        "**交叉熵損失函數（Cross Entropy Loss Function）**是機器學習和深度學習中常用的一種損失函數，特別是在分類任務中。它衡量的是模型預測的概率分佈與真實標籤的概率分佈之間的差異。\n",
        "\n",
        "對於一個分類問題，假設我們有 $ C $ 個類別，真實標籤為 $ y $（通常以 one-hot 編碼表示），模型的預測概率為 $ \\hat{y} $，則交叉熵損失函數可以表示為：\n",
        "\n",
        "$$\n",
        "\\text{CrossEntropy}(y, \\hat{y}) = -\\sum_{i=1}^{C} y_i \\log(\\hat{y}_i)\n",
        "$$\n",
        "\n",
        "其中：\n",
        "- $ y_i $ 是第 $ i $ 個類別的真實標籤（0 或 1）。\n",
        "- $ \\hat{y}_i $ 是模型預測的第 $ i $ 個類別的概率。\n",
        "\n",
        "### 二元分類的情況\n",
        "對於二元分類問題（$ C = 2 $），交叉熵損失函數可以簡化為：\n",
        "\n",
        "$$\n",
        "\\text{BinaryCrossEntropy}(y, \\hat{y}) = -\\left[ y \\log(\\hat{y}) + (1 - y) \\log(1 - \\hat{y}) \\right]\n",
        "$$\n",
        "\n",
        "其中：\n",
        "- $ y $ 是真實標籤（0 或 1）。\n",
        "- $ \\hat{y} $ 是模型預測的正類概率。\n",
        "\n",
        "### 實際應用\n",
        "在深度學習框架中（如 TensorFlow 或 PyTorch），交叉熵損失函數通常已經內建，可以直接調用。例如：\n",
        "\n",
        "- **PyTorch**:\n",
        "  ```python\n",
        "  import torch.nn as nn\n",
        "\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  loss = criterion(output, target)\n",
        "  ```\n",
        "\n",
        "- **TensorFlow/Keras**:\n",
        "  ```python\n",
        "  from tensorflow.keras.losses import CategoricalCrossentropy\n",
        "\n",
        "  criterion = CategoricalCrossentropy()\n",
        "  loss = criterion(y_true, y_pred)\n",
        "  ```\n",
        "\n",
        "### 注意事項\n",
        "1. **Softmax 函數**：在多類別分類中，通常會在模型的最後一層使用 Softmax 函數，將輸出轉換為概率分佈。\n",
        "2. **數值穩定性**：在實際計算中，為了避免對數函數的輸入為 0 或 1 導致數值不穩定，通常會對預測值進行裁剪（例如，限制在 $([ \\epsilon, 1-\\epsilon ]$) 範圍內）。\n",
        "\n",
        "希望這能幫助你理解交叉熵損失函數！如果有進一步的問題，歡迎隨時詢問。"
      ],
      "metadata": {
        "id": "HqZzv5YUBxFL"
      }
    }
  ]
}